# 7장. 분산 시스템을 위한 유일 ID 생성기 설계

분산 시스템에서 auto_Increment으로 유일 ID를 생성하는 접근법은

1. 데이터베이스 서버 한대로 요구를 담당할 수 없고,
2. 여러 데이터베이스 서버를 쓰는 경우에는 지연 시간(delay)을 낮추기가 무척 힘들 것이다.

**유일성이 보장되는 ID의 몇 가지 예를 보자.**

### 요구 사항

- ID는 유일해야 한다.
- ID는 숫자로만 구성되어야 한다.
- ID는 64비트로 표현할 수 있는 값이어야 한다.
- ID는 발급 날짜에 따라 정렬 가능해야 한다.
- 초당 10,000개의 ID를 만들 수 있어야 한다.

### 개략적 설계안 제시 및 동의 구하기

요구 사항에 따른 유일성이 보장되는 ID를 만드는 방법.

- 다중 마스터 복제 (multi-master replication)
- UUID (Universally Unique Identifier)
- 티켓 서버 (ticket server)
- 트위터 스노플레이크 접근법 (twitter snowflake)

### 다중 마스터 복제(mutli-master replication)

데이터베이스의 auto-increment를 데이터베이스 서버의 수 만큼 증가시키는 방법. 해당 서버가 생성한 이전 ID 값에 전체 서버의 수를 더한 값이 됨.

장점
- 데이터 베이스 수를 늘리면 초당 생성 가능ID 수를 늘릴 수 있음.

단점
- 여러 데이터 센터에 걸쳐 규모를 늘리기 어렵다.
- ID의 유일성이 보장되겠지만, 시간 흐름에 맞추어 커지도록 보장할 수는 없다.
- 서버를 추가하거나 삭제할 때도 잘 동작하도록 만들기 어렵다.

### UUID (Universally Unique Identifier)

컴퓨터 시스템에 저장되는 정보를 유일하게 식별하기 위한 128비트 자리 수로 아이디를 생성하는 방법.

장점
- 충돌 가능성이 자극히 낮음.
- 서버 간 조율 없이 독립적으로 생성 가능.
- 동기화 이슈 없음.

단점
- ID가 128비트로 길다.
- ID를 시간순으로 정렬할 수 없다.
- ID에 숫자 아닌 값이 포함될 수 있다.

### 티켓 서버 (ticket server)

auto_increment 기능을 갖춘 데이터베이스 서버, 즉 티켓 서버를 중앙 집중형으로 하나만 사용하는 것.

장점
- 유일성이 보장되는 오직 숫자로만 구성된 ID를 쉽게 만들 수 있다.
- 구현하기 쉽고, 중소 규모 애플리케이션에 적합하다.

단점
- 티켓 서버가 SPOF(Single-Point-of-Failure)가 된다. 서버에서 장애가 생기면 모든 시스템이 영향을 받는다.
    - 티켓 서버를 여러 대 준비하게 된다면, 데이터 동기화 같은 새로운 문제가 발생한다.

### 트위터 스노플레이크 접근법 (Twitter snowflake)

ID를 바로 생성하는 대신, 생성해야 하는 ID의 구조를 여러 절로 분할하는 방법

ID는 64비트로 표현할 수 있는 값이어야 한다. 요구 사항에 따라 아래와 같이 설계.

64비트 아이디 구조
- 1비트 (사인 비트) - 41비트 (타임스탬프) - 5비트 (데이터 센터 ID) - 5비트 (서버ID) - 12비트 (일련 번호)
- 사인(sign) 비트 : 나중을 위해 유보해주는 값
- 타임스탬프(timeStamp) : 기원 시작으로 몇 밀리초가 경과했는지를 나타내는 값
- 데이터 센터 ID : 데이터 센터 ID
- 서버 ID : 데이터 센터 당 서버 ID. 데이터 센터당 32개 서버 사용 가능
- 일련번호 : 각 서버에서 ID를 생성할 때마다 일련번호를 1만큼 증가. 1밀리초가 경과할 때마다 0으로 초기화

### 상세 설계

**요구 사항에 모두 일치하는 트위터 스노플레이크 접근법을 선택.**

Section 별 ID 생성 시기
- 데이터 센터 ID나 서버 ID는 시스템이 시작할 떄 결정되는 값.
    -  운영 중에서 바뀌지 않으나, 두개의 ID를 잘못 변경하게 되면 ID 충돌이 일어날 수 있기 때문에 작업 시 신중하게 할 것.
- 타임스탬프나 일련번호는 ID 생성기가 돌고 있는 중에 만들어지는 값.

### 타임 스탬프

타임 스탬프가 41비트일 경우 69년이 지나면 기원 시각을 바꾸거나 ID 체계를 다른 것으로 이전해야함.

### 일련번호

일련번호는 12비트이므로, 4096개의 값을 가질 수 있다. 어떤 서버가 같은 밀리초 동안 하나 이상의ID를 만들어 낸 경우에만 0보다 큰 값을 갖게 된다.


### 마무리

추가적으로 나올 수 있는 이슈
- 시계 동기화(clock synchronization) : 여러 서버가 물리적으로 독립된 여러 장비에서 실행되는 경우에는 시계를 동기화 할 수 있는 방법이 있어야함.
    - NTP(Network Time Protocol) 를 사용하여 문제를 해결하는 방법이 있음.
- 각 절(section)의 길이 최적화 : 동시성(concurrency)이 낮고 수평이 긴 애플리케이션이라면 일련번호 절의 길이를 줄이고 타임스탬프 절의 길이를 늘리는 것이 효과적
- 고가용성(high availability) : ID 생성기는 필수 불가별(mission critical) 컴포넌트이므로 아주 높은 가용성을 제공해야함.
  

# 8장. URL 단축기 설계
### URL 단축기 시스템 기능

- URL 단축 : 주어진 긴 URL을 훨씬 짧게 줄인다.
- URL 리디렉션 : 축약된 URL로 HTTP 요청이 오면 상태코드 301, 원래 URL로 안내
- 높은 가용성과 규모 확장성, 그리고 장애 감내가 요구됨

### API 엔드포인트
클라이언트는 서버가 제공하는 API엔드포인트를 통해 서버와 통신한다.

1. URL 단축용 엔드포인트 : 새 단축 URL을 생성하고자 하는 클라이언트는 이 엔드포인트에 단축할 URL을 인자로 실어서 POST요청을 보내야한다.
POST /api/v1/data/shorten
2. URL 리디렉션용 엔드포인트 : 단축 URL에 대해서 HTTP요청이 오면 원래 URL로 보내주기 위한 용도의 엔드포인트
GET /api/v1/shortUrl

### URL 리다이렉션
URL 리다이렉션을 위해서는 응답의 Location 헤더에 리다이렉션할 URL을 넣고, 301 혹은 302 HTTP Status Code를 사용하여 응답하면 된다. URL 단축키에서는 축약된 URL로 요청이 왔을 때, 기존의 URL을 Location 헤더에 넣은 후 301 혹은 302 상태코드로 응답하면 될 것이다.
여기서 301 혹은 302 응답의 차이는 다음과 같다.

img

301 상태코드는 Permanently Moved를 나타내며, 해당 URL에 대한 HTTP 요청의 책임이 영구적으로 Location 헤더의 URL로 이전된 것을 의미한다. 영구적으로 이전되었기에, 브라우저는 이 응답을 캐싱하고 다음에 같은 URL로 요청을 보낼 때 캐시된 원래 URL로 응답을 보낸다.

302 상태코드는 Found를 나타내며, 해당 URL에 대한 요청이 일시적으로 Location 헤더의 URL에서 처리되어야 한다는 뜻이다. 즉, 우리의 URL 단축기의 예시에서는 302 상태코드로 리다이렉션할 시, 클라이언트의 요청이 언제나 단축 URL 서버에 먼저 보내진 후 원래 URL로 리다이렉션된다.

이 두 방법은 각기 다른 장단점을 가진다.
301 상태코드를 쓸경우 브라우저에서 캐시처리를 하기에 서버 부하를 줄이는데 도움을 준다. 하지만 트래픽 분석이 중요할 때는 302 상태코드를 쓰는 것이 발생률이나 발생 위치를 추적하는데 더 좋을 것이다.

### URL 단축
www.tinyurl.com/{hash_value}
단축된 URL은 일반적으로 위 형태를 띌 것이다. 이는 결국 기존의 긴 URL을 해시 값으로 변환하는 것이 가장 핵심이라는 뜻이다.

이 해시 함수는 다음 요구사항을 만족해야한다.
- 입력으로 주어지는 긴 URL이 다른 값이면 해시값도 달라야한다.
- 계산된 해시 값은 원래 입력으로 조어졌떤 긴 URL로 복원될수 있어야한다.

해시함수
우선 생각해볼 점이 해시 값의 길이이다.
단축 URL인 만큼 가능한 짧을수록 좋을 것이다.
이 URL의 길이를 정하는 것은 시스템의 규모를 고려해 판단해야 한다.
예를 들어 서비스가 최대 4000억 가량의 레코드를 가진다고 가정하자.
단축 URL은 일반적으로 문자와 숫자의 조합으로 이루어지기에 한 글자당 사용할 수 있는 문자의 개수는 10 + 26 + 26 = 62개이다.
그렇다면 문자열의 길이가 n일 때, 가능한 url의 개수는 62^n이므로 길이가 7이면 요구사항을 만족하기에 충분한 값이다.
이러한 방식으로 해시 값의 길이를 우선 결정하자.

이제 해시 함수를 구현할 것인데 책에서 설명한 방식은 "해시 후 충돌 해소" 방법과 "base-62 변환" 두가지가 있다.
- 해시 후 충돌 해소 
  - 해시 후 충돌 해소는 해시를 한번 돌린 뒤, 이 값에서 내가 원하는 길이(n)만큼 추출한다. 이 값이 기존의 값과 중복된다면 기존 Url 뒤에 사전에 정의한 문자열을 추가하고 다시 이 과정을 반복한다. 이 방법을 쓰면 충돌은 해소할 수 있지만, 단축 URL을 생성할 때 한번 이상 데이터베이스 질의를 해야하므로 오버헤드가 크다.

- 진법 변환
  - 진법 변환(base conversion)은 URL 단축기를 구현할 때 흔히 사용되는 접근법 중 하나이다. 여기서 62진법을 쓰는 이유는 해시 값에 사용할 수 있는 문자의 개수가 62개이기 때문이다. base-62를 사용할 때는 0은 0으로, 9는 9로, 10은 a로, 36은 A로 대응하여 사용한다. 이 방식을 쓸때는 기존 URL 자체에 해시를 거는 것이 아니라 Unique한 정수형 ID 값에 Base-62 변환을 사용한다.

이 방식은 유니크한 ID를 사용하는 것을 전제로 하기에 중복이 발생할 일은 없다. 다만, Unique ID를 만들기 위해 이 ID를 만드는 생성기가 필요하다는 단점이 있다. 또한, ID가 1씩 증가하는 값이라고 가정하면 다음에 사용할 수 있는 단축 URL을 쉽게 유추할 수 있기에 보안상 문제가 될 수 있다.

### URL 단축기 상세 설계
- 입력으로 긴 URL을 받는다.
- 데이터베이스에 해당 URL이 있는지 검사한다.
- 데이터베이스에 있다면 해당 URL에 대한 단축 URL을 만든 적이 있는 것이므로 데이터베이스에서 해당 단축 URL을 가져와서 클라이언트에게 반환한다.
- 데이터베이스에 없는 경우, 유일ID를 생성하고, 이 ID를 데이터베이스 기본키로 사용한다.
- 62진법 변환 적용, ID를 단축 URL로 만든다.
- ID, 단축 URL, 원래 URL로 새 데이터베이스 레코드를 만든 후 단축 URL을 클라이언트에 전달한다.

# 9장. 웹크롤러설계

### 웹 크롤러

> 웹 크롤러는 로봇 또는 스파이더라고도 부릅니다.
특정 몇 개의 웹 페이지에서 시작하여 페이지에 포함된 링크를 따라
다른 페이지로 이동하면서 연속적으로 *콘텐츠를 수집합니다.

*콘텐츠 : 웹 페이지, 이미지, 비디오, PDF 파일 등

1. **검색엔진 인덱싱**:
  - **목적**: 사용자가 특정 키워드로 검색할 때 관련된 웹 페이지를 빠르게 찾기 위해 웹 페이지 내용을 분석하고 저장합니다.
  - **방법**: 웹 크롤러가 인터넷을 탐색하며 페이지의 텍스트와 링크를 분석해 인덱스를 생성합니다.
  - **이용 매체/서비스**: Google, Bing, Yahoo! 같은 검색엔진.
2. **웹 아카이빙**:
  - **목적**: 웹 페이지의 내용을 정기적으로 보관하여, 페이지가 변경되거나 사라졌을 때 이전 버전을 조회할 수 있도록 합니다.
  - **방법**: 웹 아카이브 서비스가 정기적으로 웹 페이지를 크롤링해 스냅샷을 저장합니다.
  - **이용 매체/서비스**: 인터넷 아카이브(Wayback Machine), 국립도서관 웹 아카이빙 서비스.
3. **웹 마이닝**:
  - **목적**: 웹 데이터에서 유용한 정보나 패턴을 발견하기 위해 데이터 마이닝 기술을 적용합니다. 마케팅, 사용자 행동 분석 등에 활용됩니다.
  - **방법**: 수집된 데이터에 통계적, 기계 학습적 방법을 적용해 패턴을 찾아내고 분석합니다.
  - **이용 매체/서비스**: 온라인 마케팅 회사, 소셜 미디어 플랫폼, e-커머스 사이트.
4. **웹 모니터링**:
  - **목적**: 특정 웹사이트나 웹 페이지의 변경사항을 지속적으로 감시하고, 중요한 업데이트나 조건 충족 시 알림을 받습니다.
  - **방법**: 지정된 페이지를 정기적으로 검사하며, 내용 변경이나 특정 키워드의 출현을 확인합니다.
  - **이용 매체/서비스**: 뉴스 기관, 경쟁사 분석 도구, 웹사이트 성능 모니터링 서비스.

### 웹 크롤러를 이용한 웹 크롤링과 웹 스크래핑

> 크롤러는 웹 사이트를 탐색하고 데이터를 추출하는 작업을 수행할 때
이 두 가지 작업을 통합적으로 사용할 수 있는 도구입니다.
>

- **웹 크롤링 (Web Crawling)**:
  - 크롤러는 웹 사이트를 자동으로 탐색하면서 페이지의 URL을 수집합니다.
  - 수집된 URL은 크롤러가 다음에 방문할 페이지를 결정하는 데 사용됩니다.
  - 웹 크롤링은 주로 웹 사이트의 구조를 파악하고 인덱싱하기 위한 목적으로 사용됩니다.
- **웹 스크래핑 (Web Scraping)**:
  - 크롤러가 웹 페이지에 접근하면 해당 페이지의 HTML 또는 XML 코드를 다운로드합니다.
  - 이 코드를 분석하여 필요한 데이터를 추출하고 저장합니다.
  - 웹 스크래핑은 데이터 수집, 분석, 보고서 작성 등의 목적으로 사용됩니다.

### 웹 크롤러의 기본 알고리즘과 대규모 웹 환경에서의 크롤러

> 웹 크롤러의 설계는 표면적으로 단순해 보이는 작업이지만,
실제로는 다양한 복잡한 요소들을 고려해야 하는 매우 어려운 작업입니다.
>

**기본알고리즘 :**

1. URL 집합이 입력으로 주어지면 해당 URL들이 가리키는 모든 웹 페이지를 다운로드한다.

2. 다운받은 웹 페이지에서 URL들을 추출한다.

3.추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복한다.

**대규모 웹 환경에서의 크롤러 고려사항: (뒤에서 다룸)**
4가지 주요 고려사항들은 웹 크롤러가 효과적이고 인터넷의 방대한 정보를
수집하고 처리할 수 있도록 하기 위해 생겨났습니다.

- **규모 확장성**
  - **이유**: 웹의 규모가 수시로 증가하므로, 크롤러는 대규모 데이터를 처리할 수 있는 능력이 필수적임.
  - **목적**: 크롤러가 시간이 지남에 따라 증가하는 데이터 양을 지속적으로 처리할 수 있도록, 자동화된 확장 메커니즘과 효율적인 자원 관리 기능을 갖추는 것.
  - **구체적 방안**: 클라우드 기반 인프라 사용, 자동화된 확장성 관리 시스템 구현, 분산 처리 기술 적용.
- **안정성**
  - **이유**: 웹은 다양한 형식과 예측 불가능한 문제를 포함하므로, 크롤러는 이를 견딜 수 있어야 함.
  - **목적**: 예기치 않은 오류 발생 시에도 크롤러가 계속 작동할 수 있도록, 견고한 오류 처리 및 복구 메커니즘을 갖추는 것.
  - **구체적 방안**: 오류 로깅, 자동 재시도 메커니즘, 타임아웃 및 예외 처리 전략 구현.
- **예절**
  - **이유**: 과도한 요청은 웹 서버에 부담을 줄 수 있으므로, 크롤러는 서버에 대한 존중을 유지해야 함.
  - **목적**: 웹사이트의 정상적인 운영을 방해하지 않으면서 효과적으로 데이터를 수집하기 위해, 적절한 크롤링 속도 및 접근 방법을 유지하는 것.
  - **구체적 방안**: Crawl-delay 규칙 준수, 동적 요청 속도 조정, robots.txt 준수.
- **확장성**
  - **이유**: 웹은 지속적으로 새로운 형식과 기술로 발전하고 있으므로, 크롤러는 이에 적응할 수 있어야 함.
  - **목적**: 새롭고 다양한 형태의 웹 콘텐츠를 신속하게 인식하고 처리할 수 있도록, 유연한 아키텍처 및 적응형 파싱 기능을 갖추는 것.
  - **구체적 방안**: 모듈화된 구조 설계, 플러그인이나 확장 가능한 컴포넌트를 사용한 기능 추가, 다양한 데이터 형식 처리 능력 강화.


### 2 **.  계략적 규모 추정과 설계**

> - 매달 10억 개의 웹 페이지를 다운로드 한다.
- QPS = 10억 / 30일 / 24시간 / 3600초 = 대략 400 page / sec
- 최대 (Peak) QPS = 2 X QPS = 800 page/sec
- 웹 페이지의 평균 크기는 500k라고 가정
- 10억 페이지 x 500k = 500 TB /month.
- 1개월치 데이터를 보관하는 데는 500 TB, 5년간 보관한다고
  가정하면 결국 500TB X 12개월 X 5년 = 30PB의 저장용량이 필요하다.


위의 요구사항대로 계략적 설계를 하면 아래 이미지처럼 된다.

<img width="713" alt="" src="1.png">

- **시작 URL 집합**:
  크롤러가 크롤링을 시작하는 지점입니다.
  전체 웹을 크롤링하는 경우, 다양한 출발점을 선정합니다.
  크롤러가 가능한 많은 링크를 탐색하도록 합니다.
  주제별로 시작 URL을 다르게 설정하여 URL 공간을 세분화할 수도 있습니다.
- **미수집 URL 저장소**:
  아직 방문하지 않은 URL을 관리하는 저장소입니다.
  FIFO(First-In-First-Out) 큐로 구현됩니다.
- **HTML 다운로더**:
  웹 페이지를 다운로드하는 컴포넌트입니다.
  미수집 URL 저장소로부터 URL을 받아 작업합니다.
- **도메인 이름 변환기**:
  웹 페이지를 다운로드하기 전에 URL을 해당  IP 주소로 변환해야 합니다.
- **콘텐츠 파서**:
  다운로드된 웹 페이지를 파싱하고 검증합니다.
  이상한 웹 페이지로 인한 문제를 방지하고 저장 공간을 효율적으로 사용합니다.
  독립된 컴포넌트로 구성됩니다.
- **콘텐츠 저장소**:
  HTML 문서를 저장하는 시스템으로, 다양한 요소를 고려하여 구현해야 합니다.
  대부분의 콘텐츠는 디스크에 저장하고, 인기 있는 콘텐츠는 메모리에 저장합니다.
  접근 지연시간을 줄입니다.
- **URL 추출기**:
  HTML 페이지에서 링크를 파싱하여 추출합니다.
- **URL 필터**:
  크롤링 대상에서 특정한 조건을 만족하는 URL을 제외합니다.
  예를 들어,
  특정 콘텐츠 타입이나 파일 확장자를 가진 URL,
  오류가 발생하는 URL, 접근이 제외된 URL 등을 필터링합니다.
- **이미 방문한 URL?**
  이미 방문한URL은 방문하지않고, 미수집 URL저장소로 가서
  새로운 URL을 크롤링합니다.
  ( 블룸 필터, 해시 테이블 사용됨 )
- **URL 저장소**:
  새로운 링크는 저장합니다.

*URL저장소에 접근하기전에, 미수집 URL저장소에서 방문한 URL인지 체크하는게 효율적이지않은가?

### 상세 설계

> - DFS(Depth-First Search)와 BFS(Breadth-First Search)
- 미수집 URL저장소
- HTML다운로더
- 안정성 확보 전략
- 확장성 확보 전략
- 문제있는 콘텐츠 감지 및 회피전략
>

### DFS와 BFS

> DFS(Depth-First Search)와 BFS(Breadth-First Search)는
시작 URL 집합으로부터 크롤링을 시작한 후 어떻게 웹을 탐색할지 결정하는 데 사용됩니다.

<img width="713" alt="" src="2.png">


- **BFS 방식을 사용할 경우 :**
  웹의 넓은 영역을 폭넓게 탐색하려면 BFS가 유리합니다.
  즉, BFS는 같은 레벨의 다른 URL들을 방문합니다.(병렬적 처리)
  규모가 큰 웹사이트에서는 BFS를 사용하여 레벨별로 정보를 수집하는 것이 더 효율적일 수 있습니다.
  FIFO를 사용하는데 한쪽은 탐색할 URL을 놓고, 한쪽은 꺼내기만 합니다.
  - **문제점:
    병렬적으로 처리하기때문에** 특정 시간에 많은 수의 페이지를 빠르게 요청하게 만듭니다.
    대규모 사이트나 인기 있는 서비스에서 크롤링을 할 때 해당 사이트의 서버에
    상당한 부하를 줄 수 있습니다. ( 예의없음 )
    - **속도 제한**:
      크롤러가 요청을 보내는 속도를 제한합니다. 한 시간에 서버에 보내는 요청의 수를 조절합니다.
    - **폴리트니스(politeness) 정책**:
      robots.txt 파일과 HTTP 헤더를 확인하여 사이트의 크롤링 지침을 따릅니다.
      "Crawl-delay" 지침과 같은 것을 존중합니다.
    - **동적 크롤링**:
      사이트의 응답 시간을 모니터링하고, 서버가 느려질 때 자동으로 요청 속도를 줄입니다.
- **DFS 방식을 사용할 경우 :**
  목적이 웹의 특정 부분에 대한 심층 정보를 수집하는 것이라면 DFS가 적합할 수 있습니다.
  ****크롤러는 시작 URL 집합 중 하나에서 시작하여 가능한 한 깊게 링크를 따라 내려갑니다.
  DFS는 하나의 경로를 따라 링크의 최대 깊이까지 방문한 후에야 다른 경로를 탐색합니다.
  - **문제점**:
    순환 경로(cycles) 또는 매우 깊은 경로에서 비효율적입니다.
    그리고 전체적인 구조를 파악하기 어렵습니다.
    (웹 사이트가 다양한 주제와 섹션으로 구성되어 있는지 모름)

### 미수집URL저장소 [예의]

> 우선순위와 신선도로 필터링하는
미수집 URL저장소를 잘 구현하면 예의있는 크롤러가된다.
- 예의
- 우선순위
- 신선도
- 미수집 URL저장소를 위한 지속성 저장장치
>

- **왜 예의를 자꾸 언급하는가? :**
  - **서버에 과한 요청을하여** Dos 공격으로 간주될 수 있다.
  - 해당 사이트가 마비될 수 있다.
  - 그래서 한번에 한 페이지만요청해야한다.

- **그러면 너무 느려지지않나..? :**
  - 동시에 여러 개의 크롤러 또는 크롤링 스레드를 실행합니다.
  - 각각 다른 페이지를 요청하면, 전체적인 크롤링 속도를 높일 수 있습니다.
  - 각 스레드는 독립된 FIFO 큐를 가지고 있어 서로 다른 URL을 처리합니다.


즉, 각 다운로드 스레드는 별도의 FIFO 큐를 가지고 있어서
해당 큐에서 꺼낸 URL만 다운로드한다.
그럼 한번에 한 페이지만 요청해서 큐와 스레드를 이용해서
요청은 한번이지만 해당 페이지에 많은 정보를 예의있게 크롤링 할 수 있다.

- **큐 라우터 :**
  같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장하는 역할을 한다.
- **매핑 테이블 :**
  호스트 이름과 큐 사이의 관계를 보관하는 테이블이다.
- **FIFO 큐 :**
  같은 호스트에 속한 URl은 언제나 같은 큐에 보관된다.
- **큐 선택기 :**
  큐 선택기는 큐들을 순회하면서 큐에서 URl을 꺼내서 해당 큐에서 나온 URL을 다운로드하도록 지정된 작업 스레드에 전달하는 역할을 한다.
- **작업 스레드 :**
  작업 스레드는 전달된 URL을 다운로드하는 작업을 수행한다. 전달된 URL은 순차적으로 처리될 것이며, 작업들 사이에는 일정한 지연시간을 둘 수 있다.

### 미수집URL저장소 [우선순위]

> 우선순위와 신선도로 필터링하는
미수집 URL저장소를 잘 구현하면 예의있는 크롤러가된다.
- 예의
- 우선순위
- 신선도
- 미수집 URL저장소를 위한 지속성 저장장치


- 입력된 URL이 순위결정장치를 통과하여 여러 FIFO 큐 중 하나로 배정됩니다.
- 큐 선택기가 큐에서 URL을 선택하여 크롤링을 진행합니다.
- 순위결정장치는 일반적으로 다음과 같은 기준에 기반하여 우선 순위를 결정합니다:
  - **URL의 중요성**:
    특정 페이지의 중요성이나 인기도를 기준으로 할 수 있습니다.
    예를 들어, 홈페이지나 사이트맵 같은 고정적으로 중요한 페이지들을 먼저 크롤링할 수 있습니다.
  - **콘텐츠의 신선도**:
    최근에 변경된 페이지나 자주 업데이트되는 페이지를 우선적으로 크롤링합니다.
  - **사이트의 크롤링 정책**:
    **robots.txt** 파일이나 메타 태그에서 지정한 크롤링 지침에 따라 우선 순위를 조정합니다.
  - **백링크 수**:
    다른 웹사이트로부터 링크된 횟수가 많은 페이지는 더 높은 우선 순위를 가질 수 있습니다.
  - **사용자 정의 규칙**:
    특정 주제, 키워드 또는 다른 사용자 정의 조건에 따라 우선 순위를 결정합니다.

**[완전체]**

<img width="412" alt="Untitled (1)" src="4.png">


- 전면큐(front queue) : 우선순위 결정 과정을 처리한다.
- 후면큐(back queue) : 크롤러가 예의 바르게 동작하도록 보증한다.

### 미수집 URL저장소를 위한 지속성 저장장치

> 대부분의 URL은 디스크에 두지만  IO(input/output)비용을 줄이기위해
메모리 버퍼에 큐를 둔다.
버퍼에 있는 데이터는 주기적으로 디스크에 기록해둔다.
>

### HTML다운로더 [robots.txt]

> HTML 다운로더는 HTTP 프로토콜을 통해 웹 페이지를 내려 받는다.

Robots.txt는 웹사이트의 루트 디렉토리에 위치하는 텍스트 파일로,
웹 크롤러가 사이트의 어떤 부분을 방문하거나 스캔해서는 안 되는지
지시하는 규칙을 정의합니다.

*Robots.txt 파일은
웹 사이트 소유자 또는 관리자가 선택적으로 작성함(필수는 아님)
아래처럼 작성한다.
>
>
> User-agent: [크롤러 또는 검색 엔진 이름]
> Disallow: [크롤링이 금지된 디렉토리 또는 파일 경로]
> Allow: [크롤링이 허용된 디렉토리 또는 파일 경로]
>

- **HTML 다운로더와 robots.txt의 상호작용 :**
  크롤러가 URL을 방문하기 전에, HTML 다운로더는 먼저 해당 사이트의 **robots.txt** 파일을 확인합니다.
  이 파일의 지시사항에 따라 특정 페이지를 다운로드할지 말지를 결정합니다.
  만약 **robots.txt**가 특정 경로의 크롤링을 금지하고 있다면, 예의 바른 크롤러는
  그 페이지를 다운로드해서는 안 됩니다.

### HTML다운로더 [성능 최적화]

> HTML 다운로더를 설계할 때는 성능최적화도 아주 중요하다.
>

- **분산 크롤링:**

    <img width="307" alt="Untitled (3)" src="https://github.com/organization-for-study/study-system-design-interview/assets/97773895/e51d678d-7cb4-4441-8a16-1380b2c4150b">

  - 크롤링 작업을 여러 서버로 분산시켜 성능을 향상시키는 방법입니다.
  - 각 서버는 여러 스레드를 사용하여 다운로드 작업을 처리합니다
  - URL 공간을 작은 단위로 분할하여 각 서버가 일부 다운로드를 담당한다.
- **도메인 이름 변환 결과 캐시:**
  - DNS 요청에 의한 도메인 이름 변환 작업이 크롤러의 병목 현상 중 하나
  - 그래서 DNS 조회 결과를 캐시에 저장하고 주기적으로 갱신하여 크롤러 성능을 향상시킨다.
  - 이로 인해 DNS 요청으로 인한 대기 시간을 줄일 수 있다.
- **지역성:**
  - 크롤링 서버를 지역별로 분산 배치하여 페이지 다운로드 시간을 최소화하는 방법입니다.
  - 크롤링 서버가 대상 서버와 지역적으로 가까우면 다운로드 시간이 줄어든다.
- **짧은 타임아웃:**
  - 웹 서버가 느리게 응답하거나 아예 응답하지 않는 경우에 대비해 최대 대기 시간을 설정합니다.
  - 이 시간 동안 서버가 응답하지 않으면 크롤러는 해당 페이지 다운로드를 중지하고 다음 페이지로 이동한다.

### HTML다운로더 [안정성]

> 최적화된 성능뿐 아니라 안정성도 다운로더 설계 시 중요하게 고려해야 할 부분이다.

특히, 수집할 데이터가 많으면 장애가 발생할 수 있으며, 이를 예방하기위해
예외처리를 하고, 서버부하를 줄이는 방법을 고려해야합니다.
그리고 크롤링된 데이터를 검증하는 절차도 필수적입니다.


- 안정 해시(consistent hashing)
  - 다운로더 서버들에 부하를 분산할 때 적용 가능한 기술이다.
  - 안정 해시를 사용하면, 서버가 추가되거나 제거될 때 전체 해시 테이블을 재분배할 필요 없이,
    최소한의 데이터만 재분배하여 부하를 분산할 수 있습니다.
  - 결과적으로 시스템의 확장성과 관리 용이성이 향상됩니다.
- 크롤링 상태 및 수집 데이터
  - 장애가 발생할 경우에도 쉽게 복구할 수 있도록 크롤링 상태와 수집된 데이터를
    지속적 저장장치에 기록해 두는 것이 바람직합니다.
  - 시스템 장애 발생 시 빠르게 복구할 수 있습니다.
  - 분산 데이터베이스나 파일 시스템을 활용하여 크롤링 상태를 체크포인트로 저장함으로써,
    장애 발생 시 마지막 체크포인트에서 작업을 재개할 수 있습니다.
- 예외 처리
  - 대규모 크롤링 시스템은 다양한 예외 상황에 직면할 수 있습니다.
  - 예외 발생 시 시스템의 다른 부분에 영향을 미치지 않고,
    문제를 격리하여 처리하는 로직을 구현해야 합니다.
  - 로깅, 알림, 자동 재시도 등을 포함한 견고한 에러 핸들링 전략이 필요합니다.
- 데이터 검증
  - 크롤링된 데이터의 정확성과 일관성을 보장하기 위해 데이터 검증 단계를 도입하는 것이 중요합니다.
  - 해시 합계, 체크섬, 데이터 스키마 검증 등을 통해 수집된 데이터가 손상되지 않았는지 확인합니다.
  - 데이터 손상이 발견될 경우, 시스템은 해당 데이터를 다시 크롤링하거나 수정할 수 있어야 합니다.


### HTML다운로더 [확장성]

> 최적화된 성능뿐 아니라 안정성도 다운로더 설계 시 중요하게 고려해야 할 부분이다.

특히, 수집할 데이터가 많으면 장애가 발생할 수 있으며, 이를 예방하기위해
예외처리를 하고, 서버부하를 줄이는 방법을 고려해야합니다.
그리고 크롤링된 데이터를 검증하는 절차도 필수적입니다.


<img width="609" alt="Untitled (4)" src="3.png">


- **PNG 다운로더**
  - PNG 다운로더는 웹 페이지에서 PNG 형식의 이미지 파일을 다운로드하는 특화된 모듈입니다.
  - 웹 페이지가 HTML 콘텐츠 외에도 다양한 멀티미디어 콘텐츠를 포함할 수 있기 때문에, 특정 파일 형식(이 경우 PNG)을 다루는 전용 다운로더가 필요할 수 있습니다.
  - 이 모듈은 웹 페이지의 HTML을 파싱하여 PNG 이미지의 URL을 찾고, 이를 다운로드 큐에 추가하여 실제 이미지 파일을 로컬 저장소나 데이터베이스에 저장합니다.
- **URL 추출기**
  - URL 추출기는 웹 페이지 내에서 새로운 링크를 찾아내는 컴포넌트입니다.
  - 웹 크롤러는 웹의 연결된 구조를 탐색하기 위해 페이지 내에서 다른 페이지로 연결되는 링크들을 추출합니다.
  - 추출된 URL은 크롤링 대상이 될 수 있으며, 이러한 링크들은 크롤링 범위를 확장하고 웹의 구조를 이해하는 데 중요합니다.
  - 이 모듈은 중복된 링크를 걸러내고, 아직 방문하지 않은 새로운 페이지의 URL을 큐에 추가합니다.
- **웹 모니터**
  - 웹 모니터는 흐름도에 명시적으로 언급되지는 않았지만, 일반적으로 웹 크롤링 시스템에서 중요한 역할을 하는 모듈입니다.
  - 웹 모니터는 웹사이트의 변경 사항을 추적하고, 특정 이벤트나 조건에 따라 알림을 제공할 수 있습니다.
  - 예를 들어, 웹 페이지의 내용이나 상태가 변했을 때 이를 감지하고, 필요한 조치를 취하기 위해 시스템 관리자나 다른 시스템 컴포넌트에 알림을 보낼 수 있습니다.
  - 웹 모니터는 크롤링 프로세스의 안정성을 유지하고, 시스템이 웹사이트의 최신 상태를 반영하도록 하는 데 도움을 줍니다.


### HTML다운로더 [문제 있는 콘텐츠 감지 및 회피]

> 문제 있는 콘텐츠를 정확하게 감지하고 회피하는건 크롤러 안정성과도 연관있습니다.
>

- **중복 콘텐츠**
  - 중복 콘텐츠는 웹사이트에서 동일한 내용이 다른 URL로 접근 가능할 때 발생합니다.
  - 예를 들어, 세션 ID, 사용자 추적 파라미터, 혹은 페이지 내 정렬 옵션 등에 의해 URL이 변형되어 같은 콘텐츠에 대해 수많은 URL이 생성될 수 있습니다.
  - 크롤러의 저장소와 처리 능력을 낭비하게 만듭니다.
  - 중복 콘텐츠를 식별하고 처리하기 위해 체크섬이나 해시값을 사용할 수 있습니다.
  - 다운로드된 페이지의 내용을 짧은 문자열로 변환하여 저장하고,
    새로 크롤링한 내용의 체크섬과 비교함으로써 중복 여부를 판단합니다.
    나중에 데이터를 읽을 때, 저장되었거나 전송받은 데이터에 대해 다시 체크섬을 계산하고,
    이를 원래의 체크섬과 비교합니다.
    만약 두 체크섬 값이 일치하면 데이터가 변형되지 않았음을 의미하며,
    불일치한다면 데이터가 손상되었거나 오류가 발생했음을 나타냅니다.
- **거미덫 (Spider Trap)**
  - 거미덫은 웹 크롤러가 무한히 생성되는 URL 패턴에 갇히는 상황입니다.
  - 예시로,
    - `http://example.com/page1
      http://example.com/page1/page2,
      http://example.com/page1/page2/page3`
      등과 같이 계속해서 새로운 하위 페이지가 생성되는 경우가 있습니다.
      이를 방지하기 위해 크롤러는 URL의 패턴을 분석하고, 무한히 중첩되는
      디렉토리나 파라미터를 감지하여 거미덫으로부터 벗어날 수 있도록 설계되어야 합니다.
- **데이터 노이즈**
  - 크롤링된 데이터 중 가치가 없는 정보를 데이터 노이즈라고 합니다.
  - 웹사이트의 반복적인 헤더, 푸터, 네비게이션 바와 같은 요소 또는 무관한 광고 등이 될 수 있습니다.
  - 데이터 노이즈를 제거하기 위해 크롤러는 데이터를 클리닝하는 과정을 거쳐야합니다.
  - 특정 HTML 태그를 무시하거나, 내용의 특정 패턴을 인식하여 필터링하는 규칙을 적용합니다.
  - 때때로 머신 러닝과 같은 고급 기술을 활용하여 중요한 콘텐츠를 노이즈로부터 구분하기도 합니다.